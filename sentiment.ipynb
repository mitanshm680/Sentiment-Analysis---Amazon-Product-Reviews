{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unnecessary fields\n",
    "### Description\n",
    "This Python script reads a large JSON Lines (JSONL) file line by line, processes each entry to remove the specified fields, and writes the cleaned data into a new JSONL file. The script is optimized for handling large datasets by processing entries incrementally to avoid memory overload, making it suitable for datasets containing millions of records.\n",
    "\n",
    "### Features\n",
    "Field Removal: Removes non-essential fields (images, videos, details, and features) from each entry.\n",
    "Memory Efficiency: Processes each line independently without loading the entire file into memory.\n",
    "Scalability: Capable of handling datasets with millions of entries due to its line-by-line processing approach.\n",
    "Preserves Original Structure: Maintains the integrity of the remaining data fields, ensuring the dataset is ready for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to cleaned_metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"updated_metadata.jsonl\"\n",
    "output_file = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Fields to remove\n",
    "fields_to_remove = [\"images\", \"videos\", \"details\", \"features\", \"bought_together\", \"description\"]\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())  # Load the JSON object\n",
    "        # Remove the specified fields\n",
    "        for field in fields_to_remove:\n",
    "            entry.pop(field, None)\n",
    "        # Write the cleaned entry to the output file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning reviews\n",
    "Objective:\n",
    "The purpose of this step is to preprocess the cleaned_metadata.jsonl file by removing unnecessary subfields from the reviews field. This process ensures that only the relevant information for sentiment analysis is retained in the reviews data, making it cleaner and more focused for downstream processing.\n",
    "\n",
    "Description:\n",
    "In this step, we focus on cleaning the reviews field within each entry of the cleaned_metadata.jsonl file. Specifically, we remove the following unwanted subfields from each review:\n",
    "\n",
    "parent_asin\n",
    "user_id\n",
    "asin\n",
    "helpful_vote\n",
    "These fields are irrelevant for sentiment analysis, as they do not contribute to evaluating the tone or opinion expressed in the review. By removing these fields, we reduce noise in the dataset, streamline the structure, and make it easier to analyze the sentiment of the reviews based on the remaining relevant fields, such as rating, title, text, and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews cleaned and file cleaned_metadata.jsonl updated.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths (same file for input and output)\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Fields to remove within the reviews\n",
    "review_fields_to_remove = [\"parent_asin\", \"user_id\", \"asin\", \"helpful_vote\", \"images\"]\n",
    "\n",
    "# Process the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    lines = infile.readlines()  # Read all lines into memory\n",
    "\n",
    "# Modify the data\n",
    "modified_lines = []\n",
    "for line in lines:\n",
    "    entry = json.loads(line.strip())  # Load the JSON object\n",
    "    \n",
    "    # Clean the reviews field\n",
    "    if \"reviews\" in entry and isinstance(entry[\"reviews\"], list):\n",
    "        cleaned_reviews = []\n",
    "        for review in entry[\"reviews\"]:\n",
    "            # Remove specified fields in each review\n",
    "            cleaned_review = {k: v for k, v in review.items() if k not in review_fields_to_remove}\n",
    "            cleaned_reviews.append(cleaned_review)\n",
    "        entry[\"reviews\"] = cleaned_reviews\n",
    "\n",
    "    # Prepare the modified entry for output\n",
    "    modified_lines.append(json.dumps(entry))\n",
    "\n",
    "# Overwrite the file with the modified data\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for modified_line in modified_lines:\n",
    "        outfile.write(modified_line + \"\\n\")\n",
    "\n",
    "print(f\"Reviews cleaned and file {file_path} updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing reviews that are not verified purchases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned_metadata.jsonl has been updated with filtered reviews.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "# Read the file, filter the reviews, and write back to the same file\n",
    "with open(file_path, 'r+') as file:\n",
    "    lines = file.readlines()  # Read all lines into a list\n",
    "    file.seek(0)  # Move the file pointer to the beginning\n",
    "    file.truncate()  # Clear the file content\n",
    "\n",
    "    # Process each line and filter the reviews\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse the JSON data\n",
    "        \n",
    "        # Check if 'reviews' exists, then filter the reviews based on 'verified_purchase'\n",
    "        if 'reviews' in data:\n",
    "            # Only keep reviews where 'verified_purchase' is True\n",
    "            data['reviews'] = [review for review in data['reviews'] if review.get('verified_purchase', False) is True]\n",
    "\n",
    "        # Write the cleaned data back to the same file\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"File {file_path} has been updated with filtered reviews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning data and feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert dates to suitable format and remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:15: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:19: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp converted and HTML tags removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your cleaned data\n",
    "import json\n",
    "\n",
    "# Read the cleaned metadata (ensure it's in the proper JSON format for manipulation)\n",
    "with open(\"cleaned_metadata.jsonl\", \"r\") as file:\n",
    "    cleaned_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Function to convert Unix timestamp to a readable date\n",
    "def convert_timestamp(timestamp):\n",
    "    # Convert milliseconds to seconds for datetime conversion\n",
    "    return datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to remove HTML tags from the review text\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Apply the transformations to the data\n",
    "for entry in cleaned_data:\n",
    "    # Convert timestamp for each review\n",
    "    for review in entry.get('reviews', []):\n",
    "        review['timestamp'] = convert_timestamp(review['timestamp'])\n",
    "        # Remove HTML tags in the review text\n",
    "        review['text'] = remove_html_tags(review['text'])\n",
    "\n",
    "# Save the updated data back to cleaned_metadata.jsonl\n",
    "with open(\"cleaned_metadata.jsonl\", \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Timestamp converted and HTML tags removed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding new fields such as review_count and price range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Count and Price Range features added successfully, handling null and non-numeric prices.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to add Review Count feature\n",
    "def add_review_count(data):\n",
    "    for entry in data:\n",
    "        entry['review_count'] = len(entry.get('reviews', []))  # Count reviews for each product\n",
    "    return data\n",
    "\n",
    "# Function to add Price Range feature, handling null prices and converting to numeric\n",
    "def add_price_range(data):\n",
    "    for entry in data:\n",
    "        price = entry.get('price', None)  # Get the price, default to None if not present\n",
    "        \n",
    "        # If the price is None or empty, set to 'unknown'\n",
    "        if price is None or price == '':\n",
    "            entry['price_range'] = 'unknown'  # Set as 'unknown' if price is missing or empty\n",
    "        else:\n",
    "            try:\n",
    "                # Convert price to float to ensure proper comparison\n",
    "                price = float(price)\n",
    "                if price < 10:\n",
    "                    entry['price_range'] = 'low'\n",
    "                elif 10 <= price < 30:\n",
    "                    entry['price_range'] = 'medium'\n",
    "                else:\n",
    "                    entry['price_range'] = 'high'\n",
    "            except ValueError:\n",
    "                # If conversion fails (e.g., if price is non-numeric), set it as 'unknown'\n",
    "                entry['price_range'] = 'unknown'\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the cleaned data from the cleaned_metadata.jsonl file\n",
    "with open(\"cleaned_metadata.jsonl\", \"r\") as file:\n",
    "    cleaned_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Apply Review Count and Price Range feature engineering\n",
    "cleaned_data = add_review_count(cleaned_data)\n",
    "cleaned_data = add_price_range(cleaned_data)\n",
    "\n",
    "# Save the updated data with the new features to the same file\n",
    "with open(\"cleaned_metadata.jsonl\", \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Review Count and Price Range features added successfully, handling null and non-numeric prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding sentiment scores scale -1 to 1 negative to positive, preprocessing text - Lowercasing, Remove Special Characters, Tokenization, Remove Stopwords, Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned_metadata.jsonl has been updated with sentiment scores.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keep alphanumeric and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply stemming or lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(ps.stem(word)) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to get sentiment score (VADER)\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment['compound']  # Compound score ranges from -1 (negative) to 1 (positive)\n",
    "\n",
    "# Function to adjust sentiment score based on rating\n",
    "def adjust_sentiment_based_on_rating(rating, text_sentiment):\n",
    "    # Normalize rating to scale -1 to 1 (1 is negative, 5 is positive)\n",
    "    rating_sentiment = 2 * (rating / 5) - 1\n",
    "    \n",
    "    # Combine text sentiment and rating sentiment (average them)\n",
    "    # Adjust the weights depending on how much importance you want to give to the rating vs text\n",
    "    combined_sentiment = (text_sentiment + rating_sentiment) / 2\n",
    "    return combined_sentiment\n",
    "\n",
    "# File path\n",
    "file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "# Read the file, preprocess the text, and add sentiment scores\n",
    "with open(file_path, 'r+') as file:\n",
    "    lines = file.readlines()  # Read all lines into a list\n",
    "    file.seek(0)  # Move the file pointer to the beginning\n",
    "    file.truncate()  # Clear the file content\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse the JSON data\n",
    "\n",
    "        # Check if 'reviews' exists and process each review\n",
    "        if 'reviews' in data:\n",
    "            for review in data['reviews']:\n",
    "                # Preprocess the review text\n",
    "                cleaned_text = preprocess_text(review.get('text', ''))\n",
    "                # Get sentiment score for the review text\n",
    "                text_sentiment = get_sentiment_score(cleaned_text)\n",
    "                # Adjust sentiment score based on the rating\n",
    "                final_sentiment = adjust_sentiment_based_on_rating(review.get('rating', 0), text_sentiment)\n",
    "                # Add the final sentiment score to the review\n",
    "                review['sentiment_score'] = final_sentiment\n",
    "\n",
    "        # Write the updated data back to the same file\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"File {file_path} has been updated with sentiment scores.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count of reviews and average of sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSONL file\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Process data line by line\n",
    "with open(file_path, \"r+\") as file:\n",
    "    lines = file.readlines()\n",
    "    file.seek(0)  # Reset pointer to the start of the file\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse JSON data\n",
    "        \n",
    "        # Add word_count for each review\n",
    "        if \"reviews\" in data:\n",
    "            for review in data[\"reviews\"]:\n",
    "                review[\"word_count\"] = len(review[\"text\"].split())\n",
    "        \n",
    "            # Compute average_sentiment_score\n",
    "            total_sentiment = sum(review[\"sentiment_score\"] for review in data[\"reviews\"])\n",
    "            data[\"average_sentiment_score\"] = total_sentiment / len(data[\"reviews\"]) if data[\"reviews\"] else 0.0\n",
    "        else:\n",
    "            data[\"average_sentiment_score\"] = 0.0  # Default if no reviews\n",
    "        \n",
    "        # Write updated JSON data back to file\n",
    "        file.write(json.dumps(data) + \"\\n\")\n",
    "    \n",
    "    file.truncate()  # Remove any leftover data from previous content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count of positive, negative and neutral reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Temporary list to hold processed data\n",
    "processed_data = []\n",
    "\n",
    "# Read the JSONL file line by line\n",
    "with open(file_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        # Parse each line as a JSON object\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Initialize counts\n",
    "        positive_reviews_count = 0\n",
    "        negative_reviews_count = 0\n",
    "        neutral_reviews_count = 0\n",
    "        \n",
    "        # Process reviews and count sentiment categories\n",
    "        if \"reviews\" in data:\n",
    "            for review in data[\"reviews\"]:\n",
    "                score = review.get(\"sentiment_score\", 0)\n",
    "                if score > 0.1:\n",
    "                    positive_reviews_count += 1\n",
    "                elif score < -0.1:\n",
    "                    negative_reviews_count += 1\n",
    "                else:\n",
    "                    neutral_reviews_count += 1\n",
    "        \n",
    "        # Add counts to the data\n",
    "        data[\"positive_reviews_count\"] = positive_reviews_count\n",
    "        data[\"negative_reviews_count\"] = negative_reviews_count\n",
    "        data[\"neutral_reviews_count\"] = neutral_reviews_count\n",
    "        \n",
    "        # Append updated data to the processed list\n",
    "        processed_data.append(data)\n",
    "\n",
    "# Optional: Write the updated dataset back to the same file (or print/save as needed)\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    for record in processed_data:\n",
    "        outfile.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
