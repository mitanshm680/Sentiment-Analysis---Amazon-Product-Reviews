{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unnecessary fields\n",
    "### Description\n",
    "This Python script reads a large JSON Lines (JSONL) file line by line, processes each entry to remove the specified fields, and writes the cleaned data into a new JSONL file. The script is optimized for handling large datasets by processing entries incrementally to avoid memory overload, making it suitable for datasets containing millions of records.\n",
    "\n",
    "### Features\n",
    "Field Removal: Removes non-essential fields (images, videos, details, and features) from each entry.\n",
    "Memory Efficiency: Processes each line independently without loading the entire file into memory.\n",
    "Scalability: Capable of handling datasets with millions of entries due to its line-by-line processing approach.\n",
    "Preserves Original Structure: Maintains the integrity of the remaining data fields, ensuring the dataset is ready for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to cleaned_metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"updated_metadata.jsonl\"\n",
    "output_file = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Fields to remove\n",
    "fields_to_remove = [\"images\", \"videos\", \"details\", \"features\", \"bought_together\", \"description\"]\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())  # Load the JSON object\n",
    "        # Remove the specified fields\n",
    "        for field in fields_to_remove:\n",
    "            entry.pop(field, None)\n",
    "        # Write the cleaned entry to the output file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning reviews\n",
    "Objective:\n",
    "The purpose of this step is to preprocess the cleaned_metadata.jsonl file by removing unnecessary subfields from the reviews field. This process ensures that only the relevant information for sentiment analysis is retained in the reviews data, making it cleaner and more focused for downstream processing.\n",
    "\n",
    "Description:\n",
    "In this step, we focus on cleaning the reviews field within each entry of the cleaned_metadata.jsonl file. Specifically, we remove the following unwanted subfields from each review:\n",
    "\n",
    "parent_asin\n",
    "user_id\n",
    "asin\n",
    "helpful_vote\n",
    "These fields are irrelevant for sentiment analysis, as they do not contribute to evaluating the tone or opinion expressed in the review. By removing these fields, we reduce noise in the dataset, streamline the structure, and make it easier to analyze the sentiment of the reviews based on the remaining relevant fields, such as rating, title, text, and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews cleaned and file cleaned_metadata.jsonl updated.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths (same file for input and output)\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Fields to remove within the reviews\n",
    "review_fields_to_remove = [\"parent_asin\", \"user_id\", \"asin\", \"helpful_vote\", \"images\"]\n",
    "\n",
    "# Process the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    lines = infile.readlines()  # Read all lines into memory\n",
    "\n",
    "# Modify the data\n",
    "modified_lines = []\n",
    "for line in lines:\n",
    "    entry = json.loads(line.strip())  # Load the JSON object\n",
    "    \n",
    "    # Clean the reviews field\n",
    "    if \"reviews\" in entry and isinstance(entry[\"reviews\"], list):\n",
    "        cleaned_reviews = []\n",
    "        for review in entry[\"reviews\"]:\n",
    "            # Remove specified fields in each review\n",
    "            cleaned_review = {k: v for k, v in review.items() if k not in review_fields_to_remove}\n",
    "            cleaned_reviews.append(cleaned_review)\n",
    "        entry[\"reviews\"] = cleaned_reviews\n",
    "\n",
    "    # Prepare the modified entry for output\n",
    "    modified_lines.append(json.dumps(entry))\n",
    "\n",
    "# Overwrite the file with the modified data\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for modified_line in modified_lines:\n",
    "        outfile.write(modified_line + \"\\n\")\n",
    "\n",
    "print(f\"Reviews cleaned and file {file_path} updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing reviews that are not verified purchases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned_metadata.jsonl has been updated with filtered reviews.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "# Read the file, filter the reviews, and write back to the same file\n",
    "with open(file_path, 'r+') as file:\n",
    "    lines = file.readlines()  # Read all lines into a list\n",
    "    file.seek(0)  # Move the file pointer to the beginning\n",
    "    file.truncate()  # Clear the file content\n",
    "\n",
    "    # Process each line and filter the reviews\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse the JSON data\n",
    "        \n",
    "        # Check if 'reviews' exists, then filter the reviews based on 'verified_purchase'\n",
    "        if 'reviews' in data:\n",
    "            # Only keep reviews where 'verified_purchase' is True\n",
    "            data['reviews'] = [review for review in data['reviews'] if review.get('verified_purchase', False) is True]\n",
    "\n",
    "        # Write the cleaned data back to the same file\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"File {file_path} has been updated with filtered reviews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning data and feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert dates to suitable format and remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:15: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:19: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp converted and HTML tags removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your cleaned data\n",
    "import json\n",
    "\n",
    "# Read the cleaned metadata (ensure it's in the proper JSON format for manipulation)\n",
    "with open(\"cleaned_metadata.jsonl\", \"r\") as file:\n",
    "    cleaned_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Function to convert Unix timestamp to a readable date\n",
    "def convert_timestamp(timestamp):\n",
    "    # Convert milliseconds to seconds for datetime conversion\n",
    "    return datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to remove HTML tags from the review text\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Apply the transformations to the data\n",
    "for entry in cleaned_data:\n",
    "    # Convert timestamp for each review\n",
    "    for review in entry.get('reviews', []):\n",
    "        review['timestamp'] = convert_timestamp(review['timestamp'])\n",
    "        # Remove HTML tags in the review text\n",
    "        review['text'] = remove_html_tags(review['text'])\n",
    "\n",
    "# Save the updated data back to cleaned_metadata.jsonl\n",
    "with open(\"cleaned_metadata.jsonl\", \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Timestamp converted and HTML tags removed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding new fields such as review_count and price range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Count and Price Range features added successfully, handling null and non-numeric prices.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to add Review Count feature\n",
    "def add_review_count(data):\n",
    "    for entry in data:\n",
    "        entry['review_count'] = len(entry.get('reviews', []))  # Count reviews for each product\n",
    "    return data\n",
    "\n",
    "# Function to add Price Range feature, handling null prices and converting to numeric\n",
    "def add_price_range(data):\n",
    "    for entry in data:\n",
    "        price = entry.get('price', None)  # Get the price, default to None if not present\n",
    "        \n",
    "        # If the price is None or empty, set to 'unknown'\n",
    "        if price is None or price == '':\n",
    "            entry['price_range'] = 'unknown'  # Set as 'unknown' if price is missing or empty\n",
    "        else:\n",
    "            try:\n",
    "                # Convert price to float to ensure proper comparison\n",
    "                price = float(price)\n",
    "                if price < 10:\n",
    "                    entry['price_range'] = 'low'\n",
    "                elif 10 <= price < 30:\n",
    "                    entry['price_range'] = 'medium'\n",
    "                else:\n",
    "                    entry['price_range'] = 'high'\n",
    "            except ValueError:\n",
    "                # If conversion fails (e.g., if price is non-numeric), set it as 'unknown'\n",
    "                entry['price_range'] = 'unknown'\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the cleaned data from the cleaned_metadata.jsonl file\n",
    "with open(\"cleaned_metadata.jsonl\", \"r\") as file:\n",
    "    cleaned_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Apply Review Count and Price Range feature engineering\n",
    "cleaned_data = add_review_count(cleaned_data)\n",
    "cleaned_data = add_price_range(cleaned_data)\n",
    "\n",
    "# Save the updated data with the new features to the same file\n",
    "with open(\"cleaned_metadata.jsonl\", \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Review Count and Price Range features added successfully, handling null and non-numeric prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding sentiment scores scale -1 to 1 negative to positive, preprocessing text - Lowercasing, Remove Special Characters, Tokenization, Remove Stopwords, Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned_metadata.jsonl has been updated with sentiment scores.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keep alphanumeric and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply stemming or lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(ps.stem(word)) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to get sentiment score (VADER)\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment['compound']  # Compound score ranges from -1 (negative) to 1 (positive)\n",
    "\n",
    "# Function to adjust sentiment score based on rating\n",
    "def adjust_sentiment_based_on_rating(rating, text_sentiment):\n",
    "    # Normalize rating to scale -1 to 1 (1 is negative, 5 is positive)\n",
    "    rating_sentiment = 2 * (rating / 5) - 1\n",
    "    \n",
    "    # Combine text sentiment and rating sentiment (average them)\n",
    "    # Adjust the weights depending on how much importance you want to give to the rating vs text\n",
    "    combined_sentiment = (text_sentiment + rating_sentiment) / 2\n",
    "    return combined_sentiment\n",
    "\n",
    "# File path\n",
    "file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "# Read the file, preprocess the text, and add sentiment scores\n",
    "with open(file_path, 'r+') as file:\n",
    "    lines = file.readlines()  # Read all lines into a list\n",
    "    file.seek(0)  # Move the file pointer to the beginning\n",
    "    file.truncate()  # Clear the file content\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse the JSON data\n",
    "\n",
    "        # Check if 'reviews' exists and process each review\n",
    "        if 'reviews' in data:\n",
    "            for review in data['reviews']:\n",
    "                # Preprocess the review text\n",
    "                cleaned_text = preprocess_text(review.get('text', ''))\n",
    "                # Get sentiment score for the review text\n",
    "                text_sentiment = get_sentiment_score(cleaned_text)\n",
    "                # Adjust sentiment score based on the rating\n",
    "                final_sentiment = adjust_sentiment_based_on_rating(review.get('rating', 0), text_sentiment)\n",
    "                # Add the final sentiment score to the review\n",
    "                review['sentiment_score'] = final_sentiment\n",
    "\n",
    "        # Write the updated data back to the same file\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"File {file_path} has been updated with sentiment scores.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count of reviews and average of sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSONL file\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Process data line by line\n",
    "with open(file_path, \"r+\") as file:\n",
    "    lines = file.readlines()\n",
    "    file.seek(0)  # Reset pointer to the start of the file\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse JSON data\n",
    "        \n",
    "        # Add word_count for each review\n",
    "        if \"reviews\" in data:\n",
    "            for review in data[\"reviews\"]:\n",
    "                review[\"word_count\"] = len(review[\"text\"].split())\n",
    "        \n",
    "            # Compute average_sentiment_score\n",
    "            total_sentiment = sum(review[\"sentiment_score\"] for review in data[\"reviews\"])\n",
    "            data[\"average_sentiment_score\"] = total_sentiment / len(data[\"reviews\"]) if data[\"reviews\"] else 0.0\n",
    "        else:\n",
    "            data[\"average_sentiment_score\"] = 0.0  # Default if no reviews\n",
    "        \n",
    "        # Write updated JSON data back to file\n",
    "        file.write(json.dumps(data) + \"\\n\")\n",
    "    \n",
    "    file.truncate()  # Remove any leftover data from previous content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count of positive, negative and neutral reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Temporary list to hold processed data\n",
    "processed_data = []\n",
    "\n",
    "# Read the JSONL file line by line\n",
    "with open(file_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        # Parse each line as a JSON object\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Initialize counts\n",
    "        positive_reviews_count = 0\n",
    "        negative_reviews_count = 0\n",
    "        neutral_reviews_count = 0\n",
    "        \n",
    "        # Process reviews and count sentiment categories\n",
    "        if \"reviews\" in data:\n",
    "            for review in data[\"reviews\"]:\n",
    "                score = review.get(\"sentiment_score\", 0)\n",
    "                if score > 0.1:\n",
    "                    positive_reviews_count += 1\n",
    "                elif score < -0.1:\n",
    "                    negative_reviews_count += 1\n",
    "                else:\n",
    "                    neutral_reviews_count += 1\n",
    "        \n",
    "        # Add counts to the data\n",
    "        data[\"positive_reviews_count\"] = positive_reviews_count\n",
    "        data[\"negative_reviews_count\"] = negative_reviews_count\n",
    "        data[\"neutral_reviews_count\"] = neutral_reviews_count\n",
    "        \n",
    "        # Append updated data to the processed list\n",
    "        processed_data.append(data)\n",
    "\n",
    "# Optional: Write the updated dataset back to the same file (or print/save as needed)\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    for record in processed_data:\n",
    "        outfile.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General Sentiment Analysis\n",
    "- Distribution of sentiment scores (mean, median, standard deviation, etc.).\n",
    "- Proportion of positive, neutral, and negative reviews.\n",
    "-  common sentiment range (e.g., highly positive, slightly negative).\n",
    "- Comparison of sentiment scores across product categories or subcategories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentiment Distribution ===\n",
      "Total Reviews: 3279585\n",
      "Mean Sentiment Score: 0.6847\n",
      "Median Sentiment Score: 0.7881\n",
      "Standard Deviation of Sentiment Score: 0.2922\n",
      "\n",
      "=== Proportion of Reviews ===\n",
      "Positive Reviews: 94.28%\n",
      "Negative Reviews: 3.11%\n",
      "Neutral Reviews: 2.62%\n",
      "\n",
      "=== Most Common Sentiment Range ===\n",
      "The most common sentiment range is: Highly Positive\n",
      "\n",
      "=== Sentiment Range Distribution ===\n",
      "╒═══════════════════╤═════════╕\n",
      "│ Sentiment Range   │  Count  │\n",
      "╞═══════════════════╪═════════╡\n",
      "│ Highly Negative   │  20871  │\n",
      "├───────────────────┼─────────┤\n",
      "│ Negative          │  80982  │\n",
      "├───────────────────┼─────────┤\n",
      "│ Neutral           │  85832  │\n",
      "├───────────────────┼─────────┤\n",
      "│ Positive          │ 293436  │\n",
      "├───────────────────┼─────────┤\n",
      "│ Highly Positive   │ 2798464 │\n",
      "╘═══════════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "# File path to the dataset\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Sentiment binning for most common sentiment range\n",
    "sentiment_bins = [-1.0, -0.5, -0.1, 0.1, 0.5, 1.0]  # Binning the sentiment scores\n",
    "bin_labels = [\"Highly Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Highly Positive\"]\n",
    "\n",
    "# Variables to store overall sentiment analysis\n",
    "all_sentiment_scores = []\n",
    "positive_reviews_count = 0\n",
    "negative_reviews_count = 0\n",
    "neutral_reviews_count = 0\n",
    "range_counts = defaultdict(int)\n",
    "total_reviews = 0\n",
    "\n",
    "# Reading and processing the data\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        total_reviews += data.get('review_count', 0)  # Add total reviews for each product\n",
    "\n",
    "        for review in data.get('reviews', []):\n",
    "            score = review['sentiment_score']\n",
    "            all_sentiment_scores.append(score)\n",
    "            \n",
    "            # Count the positive, negative, and neutral reviews\n",
    "            if score > 0.1:\n",
    "                positive_reviews_count += 1\n",
    "            elif score < -0.1:\n",
    "                negative_reviews_count += 1\n",
    "            else:\n",
    "                neutral_reviews_count += 1\n",
    "\n",
    "            # Bin sentiment score and categorize the review\n",
    "            bin_index = np.digitize([score], sentiment_bins)[0] - 1\n",
    "            bin_index = max(0, min(bin_index, len(bin_labels) - 1))  # Ensure the index is within range\n",
    "            range_counts[bin_labels[bin_index]] += 1\n",
    "\n",
    "# 1. Distribution of Sentiment Scores\n",
    "mean_score = np.mean(all_sentiment_scores)\n",
    "median_score = np.median(all_sentiment_scores)\n",
    "std_dev_score = np.std(all_sentiment_scores)\n",
    "\n",
    "# 2. Proportion of Positive, Neutral, and Negative Reviews\n",
    "positive_percentage = (positive_reviews_count / total_reviews) * 100\n",
    "negative_percentage = (negative_reviews_count / total_reviews) * 100\n",
    "neutral_percentage = (neutral_reviews_count / total_reviews) * 100\n",
    "\n",
    "# 3. Most Common Sentiment Range (using the bin labels)\n",
    "most_common_range = max(range_counts, key=range_counts.get)\n",
    "\n",
    "# Displaying the results in a neat, tabular format\n",
    "print(\"\\n=== Sentiment Distribution ===\")\n",
    "print(f\"Total Reviews: {total_reviews}\")\n",
    "print(f\"Mean Sentiment Score: {mean_score:.4f}\")\n",
    "print(f\"Median Sentiment Score: {median_score:.4f}\")\n",
    "print(f\"Standard Deviation of Sentiment Score: {std_dev_score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Proportion of Reviews ===\")\n",
    "print(f\"Positive Reviews: {positive_percentage:.2f}%\")\n",
    "print(f\"Negative Reviews: {negative_percentage:.2f}%\")\n",
    "print(f\"Neutral Reviews: {neutral_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Most Common Sentiment Range ===\")\n",
    "print(f\"The most common sentiment range is: {most_common_range}\")\n",
    "\n",
    "# Display Sentiment Range Distribution\n",
    "print(\"\\n=== Sentiment Range Distribution ===\")\n",
    "range_table = []\n",
    "for label in bin_labels:\n",
    "    range_table.append([label, range_counts[label]])\n",
    "\n",
    "print(tabulate(range_table, headers=[\"Sentiment Range\", \"Count\"], tablefmt=\"fancy_grid\", numalign=\"center\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Category Comparison (Top 5 Categories with Highest Positive Reviews) ===\n",
      "Category        Mean Sentiment    Median Sentiment    Positive Reviews (%)    Negative Reviews (%)    Neutral Reviews (%)\n",
      "Voluntaries          0.7                0.73                  100                      0                       0\n",
      "Grounds              0.81               0.86                  100                      0                       0\n",
      "Scottish Folk        0.73               0.81                  100                      0                       0\n",
      "Zouk                 0.76               0.81                  100                      0                       0\n",
      "Nicaragua            0.71               0.73                  100                      0                       0\n",
      "\n",
      "=== Category Comparison (Top 5 Categories with Highest Negative Reviews) ===\n",
      "Category              Mean Sentiment    Median Sentiment    Positive Reviews (%)    Negative Reviews (%)    Neutral Reviews (%)\n",
      "Tierra Caliente            0.13               0.1                    50                      50                      0\n",
      "Cobra Entertainment        0.39               0.5                    60                      20                     20\n",
      "Radio Shows                0.44               0.52                76.8559                 17.0306                 6.11354\n",
      "Sleep                      0.27               0.21                57.1429                 14.2857                 28.5714\n",
      "Karaoke                    0.53               0.71                80.8253                 12.0879                 7.08679\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "\n",
    "# File path to the large dataset\n",
    "file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "# Initialize the category sentiment dictionary\n",
    "category_sentiments = defaultdict(list)\n",
    "\n",
    "# Function to process data incrementally from a file\n",
    "def process_large_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read the file line by line (assuming each line is a separate JSON object)\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Parse each line as a JSON object\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Process the sentiment scores for each category in the data\n",
    "                for review in data.get('reviews', []):\n",
    "                    sentiment_score = review.get('sentiment_score', 0.0)\n",
    "                    for category in data.get('categories', []):\n",
    "                        category_sentiments[category].append(sentiment_score)\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle error if the line is not a valid JSON object\n",
    "                print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "# Function to calculate and display the statistics\n",
    "def display_statistics():\n",
    "    category_comparison = []\n",
    "\n",
    "    # Calculate statistics for each category\n",
    "    for category, sentiments in category_sentiments.items():\n",
    "        mean_cat_score = np.mean(sentiments)\n",
    "        median_cat_score = np.median(sentiments)\n",
    "        positive_count = sum(1 for score in sentiments if score > 0.1)\n",
    "        negative_count = sum(1 for score in sentiments if score < -0.1)\n",
    "        neutral_count = sum(1 for score in sentiments if -0.1 <= score <= 0.1)\n",
    "        total_count = len(sentiments)\n",
    "\n",
    "        positive_percentage = (positive_count / total_count) * 100\n",
    "        negative_percentage = (negative_count / total_count) * 100\n",
    "        neutral_percentage = (neutral_count / total_count) * 100\n",
    "        \n",
    "        category_comparison.append({\n",
    "            'Category': category,\n",
    "            'Mean Sentiment': f\"{mean_cat_score:.2f}\",\n",
    "            'Median Sentiment': f\"{median_cat_score:.2f}\",\n",
    "            'Positive Reviews (%)': positive_percentage,\n",
    "            'Negative Reviews (%)': negative_percentage,\n",
    "            'Neutral Reviews (%)': neutral_percentage,\n",
    "        })\n",
    "\n",
    "    # Sort and display the top 5 categories with highest positive and negative reviews\n",
    "    top_positive_categories = sorted(\n",
    "        [entry for entry in category_comparison if entry['Positive Reviews (%)'] > 0],\n",
    "        key=lambda x: x['Positive Reviews (%)'], reverse=True\n",
    "    )[:5]\n",
    "\n",
    "    top_negative_categories = sorted(\n",
    "        [entry for entry in category_comparison if entry['Negative Reviews (%)'] > 0],\n",
    "        key=lambda x: x['Negative Reviews (%)'], reverse=True\n",
    "    )[:5]\n",
    "\n",
    "    # Display results without truncation\n",
    "    print(\"\\n=== Category Comparison (Top 5 Categories with Highest Positive Reviews) ===\")\n",
    "    headers = ['Category', 'Mean Sentiment', 'Median Sentiment', 'Positive Reviews (%)', 'Negative Reviews (%)', 'Neutral Reviews (%)']\n",
    "    positive_category_table = [list(item.values()) for item in top_positive_categories]\n",
    "    print(tabulate(positive_category_table, headers=headers, tablefmt=\"plain\", numalign=\"center\"))\n",
    "\n",
    "    print(\"\\n=== Category Comparison (Top 5 Categories with Highest Negative Reviews) ===\")\n",
    "    negative_category_table = [list(item.values()) for item in top_negative_categories]\n",
    "    print(tabulate(negative_category_table, headers=headers, tablefmt=\"plain\", numalign=\"center\"))\n",
    "\n",
    "# Process the data incrementally\n",
    "process_large_data(file_path)\n",
    "\n",
    "# Once the data is processed, display the results\n",
    "display_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rating Analysis\n",
    "- Distribution of ratings (1-5).\n",
    "- Frequency of each rating score (1-5).\n",
    "- Proportion of reviews with the highest and lowest ratings.\n",
    "- Average sentiment score for each rating level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Distribution of Ratings (1-5) ===\n",
      "Rating 1.0: 106123 reviews\n",
      "Rating 5.0: 2554415 reviews\n",
      "Rating 4.0: 376301 reviews\n",
      "Rating 3.0: 165893 reviews\n",
      "Rating 2.0: 76853 reviews\n",
      "\n",
      "=== Frequency of Each Rating Score (1-5) ===\n",
      "Rating 1.0: 106123 reviews\n",
      "Rating 5.0: 2554415 reviews\n",
      "Rating 4.0: 376301 reviews\n",
      "Rating 3.0: 165893 reviews\n",
      "Rating 2.0: 76853 reviews\n",
      "\n",
      "=== Proportion of Reviews with Highest and Lowest Ratings ===\n",
      "Proportion of Reviews with Highest Rating (5): 77.89%\n",
      "Proportion of Reviews with Lowest Rating (1): 3.24%\n",
      "\n",
      "=== Average Sentiment Score for Each Rating Level ===\n",
      "Rating 1.0: -0.2435\n",
      "Rating 5.0: 0.7811\n",
      "Rating 4.0: 0.5845\n",
      "Rating 3.0: 0.3174\n",
      "Rating 2.0: 0.0450\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary to store counts and sums for the analysis\n",
    "rating_counts = defaultdict(int)\n",
    "sentiment_sum = defaultdict(float)\n",
    "\n",
    "# File path to the JSONL file\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Read the file line by line to process it efficiently\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line of the JSONL file\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Extract reviews data from each entry\n",
    "        reviews = data.get('reviews', [])\n",
    "        \n",
    "        for review in reviews:\n",
    "            rating = review.get('rating')\n",
    "            sentiment_score = review.get('sentiment_score')\n",
    "            \n",
    "            # Update rating counts and sentiment sums\n",
    "            if rating is not None and sentiment_score is not None:\n",
    "                rating_counts[rating] += 1\n",
    "                sentiment_sum[rating] += sentiment_score\n",
    "\n",
    "# 1. Distribution of Ratings (1-5)\n",
    "print(\"=== Distribution of Ratings (1-5) ===\")\n",
    "for rating, count in rating_counts.items():\n",
    "    print(f\"Rating {rating}: {count} reviews\")\n",
    "\n",
    "# 2. Frequency of Each Rating Score (1-5)\n",
    "print(\"\\n=== Frequency of Each Rating Score (1-5) ===\")\n",
    "for rating, count in rating_counts.items():\n",
    "    print(f\"Rating {rating}: {count} reviews\")\n",
    "\n",
    "# 3. Proportion of Reviews with Highest and Lowest Ratings\n",
    "highest_rating_count = rating_counts.get(5.0, 0)\n",
    "lowest_rating_count = rating_counts.get(1.0, 0)\n",
    "total_reviews = sum(rating_counts.values())\n",
    "\n",
    "highest_rating_proportion = (highest_rating_count / total_reviews) * 100\n",
    "lowest_rating_proportion = (lowest_rating_count / total_reviews) * 100\n",
    "\n",
    "print(f\"\\n=== Proportion of Reviews with Highest and Lowest Ratings ===\")\n",
    "print(f\"Proportion of Reviews with Highest Rating (5): {highest_rating_proportion:.2f}%\")\n",
    "print(f\"Proportion of Reviews with Lowest Rating (1): {lowest_rating_proportion:.2f}%\")\n",
    "\n",
    "# 4. Average Sentiment Score for Each Rating Level\n",
    "print(\"\\n=== Average Sentiment Score for Each Rating Level ===\")\n",
    "for rating in rating_counts:\n",
    "    avg_sentiment = sentiment_sum[rating] / rating_counts[rating]\n",
    "    print(f\"Rating {rating}: {avg_sentiment:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review Text Analysis\n",
    "- Average, median, and maximum word count across all reviews.\n",
    "- Most common words in positive, negative, and neutral reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Review Text Analysis ===\n",
      "Average Word Count: 18.22\n",
      "Median Word Count: 8\n",
      "Maximum Word Count: 4249\n",
      "\n",
      "Most Common Words in Positive Reviews:\n",
      "great: 564076\n",
      "like: 495541\n",
      "love: 421432\n",
      "good: 377445\n",
      "Great: 301923\n",
      "best: 233219\n",
      "Love: 189600\n",
      "favorite: 141014\n",
      "fan: 124158\n",
      "never: 123481\n",
      "\n",
      "Most Common Words in Negative Reviews:\n",
      "like: 18438\n",
      "quality: 9334\n",
      "Not: 9160\n",
      "bad: 8091\n",
      "disappointed: 8028\n",
      "good: 7664\n",
      "never: 4913\n",
      "poor: 4408\n",
      "disappointed.: 4239\n",
      "version: 3965\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords if not already installed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of stopwords from NLTK\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords relevant to your dataset (optional)\n",
    "custom_stopwords = {\"album\", \"music\", \"cd\", \"track\", \"song\", \"The\", \"this\", \n",
    "                    \"that\", \"it\", \"is\", \"was\", \"and\", \"in\", \"on\", \"for\", \"with\", \n",
    "                    \"by\", \"to\", \"of\", \"a\", \"an\", \"i\", \"you\", \"we\", \"they\", \"I\", \n",
    "                    \"the\", \"It\", \"This\", \"this\", \"CD\", \"would\", \"Would\", \"one\", \"songs\", \n",
    "                    \"-\", \"it.\", \"get\", \"record\", \"first\", \"A\", \"sound\", \"even\", \n",
    "                    \"vinyl\", \"case\", \"if\", \"If\", \"time\", \"play\", \"much\", \"bought\", \n",
    "                    \"got\", \"original\", \"album.\", \"new\", \"I'm\", \"music.\", \"CD.\", \"two\", \n",
    "                    \"buy\", \"Very\", \"2\", \"disc\", \"still\", \"thought\", \"listen\", \"really\",\n",
    "                    \"many\", \"back\", \"could\", \"came\", \"received\", \"sounds\", \"know\",\n",
    "                    \"tracks\", \"recording\", \"heard\", \"It's\", \"return\", \"band\", \"well\", \n",
    "                    \"My\", \"listening\" ,\"ordered\", \"&\", \"also\", \"There\", \"think\"}\n",
    "stopwords_set.update(custom_stopwords)\n",
    "\n",
    "# Initialize counters and lists\n",
    "word_counts = []\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "# File path to the JSONL file\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Read the file line by line to process it efficiently\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line of the JSONL file\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Extract reviews data from each entry\n",
    "        reviews = data.get('reviews', [])\n",
    "        \n",
    "        for review in reviews:\n",
    "            text = review.get('text', '')\n",
    "            sentiment_score = review.get('sentiment_score')\n",
    "            \n",
    "            # Clean the review text and filter out stopwords (case-insensitive)\n",
    "            tokens = [word for word in text.lower().split() if word not in stopwords_set]\n",
    "            \n",
    "            # Calculate word count after filtering stopwords\n",
    "            word_count = len(tokens)\n",
    "            word_counts.append(word_count)\n",
    "            \n",
    "            # Categorize reviews based on sentiment score\n",
    "            if sentiment_score > 0:\n",
    "                positive_reviews.append(text)\n",
    "            elif sentiment_score < 0:\n",
    "                negative_reviews.append(text)\n",
    "\n",
    "# 1. Average, Median, and Maximum Word Count Across All Reviews\n",
    "avg_word_count = sum(word_counts) / len(word_counts)\n",
    "median_word_count = sorted(word_counts)[len(word_counts) // 2]\n",
    "max_word_count = max(word_counts)\n",
    "\n",
    "print(\"=== Review Text Analysis ===\")\n",
    "print(f\"Average Word Count: {avg_word_count:.2f}\")\n",
    "print(f\"Median Word Count: {median_word_count}\")\n",
    "print(f\"Maximum Word Count: {max_word_count}\")\n",
    "\n",
    "# 2. Most Common Words in Positive, Negative, and Neutral Reviews\n",
    "def most_common_words(reviews):\n",
    "    all_words = [word for review in reviews for word in review.split() if word not in stopwords_set]\n",
    "    word_count = Counter(all_words)\n",
    "    return word_count.most_common(10)\n",
    "\n",
    "positive_common_words = most_common_words(positive_reviews)\n",
    "negative_common_words = most_common_words(negative_reviews)\n",
    "\n",
    "print(\"\\nMost Common Words in Positive Reviews:\")\n",
    "for word, count in positive_common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nMost Common Words in Negative Reviews:\")\n",
    "for word, count in negative_common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
