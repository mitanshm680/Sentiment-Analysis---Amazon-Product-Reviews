{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unnecessary fields\n",
    "### Description\n",
    "This Python script reads a large JSON Lines (JSONL) file line by line, processes each entry to remove the specified fields, and writes the cleaned data into a new JSONL file. The script is optimized for handling large datasets by processing entries incrementally to avoid memory overload, making it suitable for datasets containing millions of records.\n",
    "\n",
    "### Features\n",
    "Field Removal: Removes non-essential fields (images, videos, details, and features) from each entry.\n",
    "Memory Efficiency: Processes each line independently without loading the entire file into memory.\n",
    "Scalability: Capable of handling datasets with millions of entries due to its line-by-line processing approach.\n",
    "Preserves Original Structure: Maintains the integrity of the remaining data fields, ensuring the dataset is ready for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to cleaned_metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"updated_metadata.jsonl\"\n",
    "output_file = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Fields to remove\n",
    "fields_to_remove = [\"images\", \"videos\", \"details\", \"features\", \"bought_together\", \"description\"]\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())  # Load the JSON object\n",
    "        # Remove the specified fields\n",
    "        for field in fields_to_remove:\n",
    "            entry.pop(field, None)\n",
    "        # Write the cleaned entry to the output file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning reviews\n",
    "Objective:\n",
    "The purpose of this step is to preprocess the cleaned_metadata.jsonl file by removing unnecessary subfields from the reviews field. This process ensures that only the relevant information for sentiment analysis is retained in the reviews data, making it cleaner and more focused for downstream processing.\n",
    "\n",
    "Description:\n",
    "In this step, we focus on cleaning the reviews field within each entry of the cleaned_metadata.jsonl file. Specifically, we remove the following unwanted subfields from each review:\n",
    "\n",
    "parent_asin\n",
    "user_id\n",
    "asin\n",
    "helpful_vote\n",
    "These fields are irrelevant for sentiment analysis, as they do not contribute to evaluating the tone or opinion expressed in the review. By removing these fields, we reduce noise in the dataset, streamline the structure, and make it easier to analyze the sentiment of the reviews based on the remaining relevant fields, such as rating, title, text, and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews cleaned and file cleaned_metadata.jsonl updated.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths (same file for input and output)\n",
    "file_path = \"cleaned_metadata.jsonl\"\n",
    "\n",
    "# Fields to remove within the reviews\n",
    "review_fields_to_remove = [\"parent_asin\", \"user_id\", \"asin\", \"helpful_vote\", \"images\"]\n",
    "\n",
    "# Process the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    lines = infile.readlines()  # Read all lines into memory\n",
    "\n",
    "# Modify the data\n",
    "modified_lines = []\n",
    "for line in lines:\n",
    "    entry = json.loads(line.strip())  # Load the JSON object\n",
    "    \n",
    "    # Clean the reviews field\n",
    "    if \"reviews\" in entry and isinstance(entry[\"reviews\"], list):\n",
    "        cleaned_reviews = []\n",
    "        for review in entry[\"reviews\"]:\n",
    "            # Remove specified fields in each review\n",
    "            cleaned_review = {k: v for k, v in review.items() if k not in review_fields_to_remove}\n",
    "            cleaned_reviews.append(cleaned_review)\n",
    "        entry[\"reviews\"] = cleaned_reviews\n",
    "\n",
    "    # Prepare the modified entry for output\n",
    "    modified_lines.append(json.dumps(entry))\n",
    "\n",
    "# Overwrite the file with the modified data\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for modified_line in modified_lines:\n",
    "        outfile.write(modified_line + \"\\n\")\n",
    "\n",
    "print(f\"Reviews cleaned and file {file_path} updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing reviews that are not verified purchases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "# Read the file, filter the reviews, and write back to the same file\n",
    "with open(file_path, 'r+') as file:\n",
    "    lines = file.readlines()  # Read all lines into a list\n",
    "    file.seek(0)  # Move the file pointer to the beginning\n",
    "    file.truncate()  # Clear the file content\n",
    "\n",
    "    # Process each line and filter the reviews\n",
    "    for line in lines:\n",
    "        data = json.loads(line)  # Parse the JSON data\n",
    "        \n",
    "        # Check if 'reviews' exists, then filter the reviews based on 'verified_purchase'\n",
    "        if 'reviews' in data:\n",
    "            # Only keep reviews where 'verified_purchase' is True\n",
    "            data['reviews'] = [review for review in data['reviews'] if review.get('verified_purchase', False) is True]\n",
    "\n",
    "        # Write the cleaned data back to the same file\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"File {file_path} has been updated with filtered reviews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning data and feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert dates to suitable format and remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:15: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\mitan\\AppData\\Local\\Temp\\ipykernel_28984\\3263700859.py:19: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp converted and HTML tags removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your cleaned data\n",
    "import json\n",
    "\n",
    "# Read the cleaned metadata (ensure it's in the proper JSON format for manipulation)\n",
    "with open(\"cleaned_metadata.jsonl\", \"r\") as file:\n",
    "    cleaned_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Function to convert Unix timestamp to a readable date\n",
    "def convert_timestamp(timestamp):\n",
    "    # Convert milliseconds to seconds for datetime conversion\n",
    "    return datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to remove HTML tags from the review text\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Apply the transformations to the data\n",
    "for entry in cleaned_data:\n",
    "    # Convert timestamp for each review\n",
    "    for review in entry.get('reviews', []):\n",
    "        review['timestamp'] = convert_timestamp(review['timestamp'])\n",
    "        # Remove HTML tags in the review text\n",
    "        review['text'] = remove_html_tags(review['text'])\n",
    "\n",
    "# Save the updated data back to cleaned_metadata.jsonl\n",
    "with open(\"cleaned_metadata.jsonl\", \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Timestamp converted and HTML tags removed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding new fields such as review_count and price range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Count and Price Range features added successfully, handling null and non-numeric prices.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to add Review Count feature\n",
    "def add_review_count(data):\n",
    "    for entry in data:\n",
    "        entry['review_count'] = len(entry.get('reviews', []))  # Count reviews for each product\n",
    "    return data\n",
    "\n",
    "# Function to add Price Range feature, handling null prices and converting to numeric\n",
    "def add_price_range(data):\n",
    "    for entry in data:\n",
    "        price = entry.get('price', None)  # Get the price, default to None if not present\n",
    "        \n",
    "        # If the price is None or empty, set to 'unknown'\n",
    "        if price is None or price == '':\n",
    "            entry['price_range'] = 'unknown'  # Set as 'unknown' if price is missing or empty\n",
    "        else:\n",
    "            try:\n",
    "                # Convert price to float to ensure proper comparison\n",
    "                price = float(price)\n",
    "                if price < 10:\n",
    "                    entry['price_range'] = 'low'\n",
    "                elif 10 <= price < 30:\n",
    "                    entry['price_range'] = 'medium'\n",
    "                else:\n",
    "                    entry['price_range'] = 'high'\n",
    "            except ValueError:\n",
    "                # If conversion fails (e.g., if price is non-numeric), set it as 'unknown'\n",
    "                entry['price_range'] = 'unknown'\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the cleaned data from the cleaned_metadata.jsonl file\n",
    "with open(\"cleaned_metadata.jsonl\", \"r\") as file:\n",
    "    cleaned_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Apply Review Count and Price Range feature engineering\n",
    "cleaned_data = add_review_count(cleaned_data)\n",
    "cleaned_data = add_price_range(cleaned_data)\n",
    "\n",
    "# Save the updated data with the new features to the same file\n",
    "with open(\"cleaned_metadata.jsonl\", \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Review Count and Price Range features added successfully, handling null and non-numeric prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding sentiment scores, preprocessing text - Lowercasing, Remove Special Characters, Tokenization, Remove Stopwords, Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m product_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Process reviews for each product\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproduct_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     57\u001b[0m     original_text \u001b[38;5;241m=\u001b[39m review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Preprocess the review text and update the review text with preprocessed text\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'reviews'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove Special Characters (punctuation, etc.)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Reconstruct the text from tokens\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Function to get sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    # Using TextBlob to get sentiment score\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity  # Returns a score between -1 and 1\n",
    "    return sentiment_score\n",
    "\n",
    "# Open the cleaned_metadata.jsonl file and process each line\n",
    "input_file_path = 'cleaned_metadata.jsonl'\n",
    "\n",
    "with open(input_file_path, 'r+') as infile:\n",
    "    lines = infile.readlines()\n",
    "    \n",
    "    # Go through each line and process\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Load the current product's metadata\n",
    "        product_data = json.loads(line.strip())\n",
    "        \n",
    "        # Process reviews for each product\n",
    "        for review in product_data['reviews']:\n",
    "            original_text = review['text']\n",
    "            \n",
    "            # Preprocess the review text and update the review text with preprocessed text\n",
    "            preprocessed_text = preprocess_text(original_text)\n",
    "            review['text'] = preprocessed_text  # Replace original text with preprocessed text\n",
    "            \n",
    "            # Get sentiment score and add it as a new field\n",
    "            sentiment_score = get_sentiment_score(preprocessed_text)\n",
    "            review['sentiment_score'] = sentiment_score\n",
    "        \n",
    "        # Update the line with the modified product data\n",
    "        lines[idx] = json.dumps(product_data) + '\\n'\n",
    "    \n",
    "    # Move the cursor to the beginning and overwrite the file\n",
    "    infile.seek(0)\n",
    "    infile.writelines(lines)\n",
    "\n",
    "print(\"Processing complete. The file has been updated.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
